{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b165e714-fe7c-42ee-b6c8-8cfce2b272c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from translating_the_law.downloading.get_data import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c875692-ff96-4c12-b07b-578da051802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_data = get_data(num = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9345df9-0989-43d1-bace-a1e437669429",
   "metadata": {},
   "source": [
    "Basic data is a list and each item inside the list is a dictionary with these keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b8681-cbc9-4637-b33a-a11e65f62cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db787329-1e5b-4131-ba53-5bb79c50fcd0",
   "metadata": {},
   "source": [
    "and then inside those keys dictionaries with these keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2ffd5-6210-48b5-bb15-fea063478cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_1 = basic_data[0][\"judgement\"]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14d92b-b6c0-4c02-8a02-eb126ca4f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import common_texts\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93cd13d",
   "metadata": {},
   "source": [
    "#### Copy dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb64c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "judg_data = [basic_data[i][\"judgement\"]['body'] for i in range(len(basic_data))]\n",
    "dfc = pd.DataFrame(judg_data).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c03a8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ca5d5",
   "metadata": {},
   "source": [
    "#### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ecf16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa40dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = dfc.rename(columns={0 : 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation: \n",
    "        text = text.replace(punctuation, ' ') \n",
    "    return text\n",
    "\n",
    "dfc['clean_text'] = dfc.text.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e789e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase (text): \n",
    "    lowercased = text.lower() \n",
    "    return lowercased\n",
    "\n",
    "dfc['clean_text'] = dfc.clean_text.apply(lowercase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b862d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaner(text, **kwargs):\n",
    "  \"\"\"params is a list of things to remove: codec, acronyms, numbers, brackets\"\"\"\n",
    "  if 'codec' in kwargs['params']:\n",
    "    text_encoded = text.encode('ascii', errors = 'ignore')\n",
    "    text_decode = text_encoded.decode()\n",
    "    clean_text = \" \".join([word for word in text_decode.split()])\n",
    "    text = clean_text\n",
    "  if 'numbers' in kwargs['params']:\n",
    "    pattern = r'[0-9]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "  if 'brackets' in kwargs['params']: \n",
    "    text = re.sub('\\(.*?\\)', '', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "  if 'acronyms' in kwargs['params']:\n",
    "    text = text.split()\n",
    "    clean_text = []\n",
    "    for word in text: \n",
    "      if any(l.islower() for l in word):\n",
    "        clean_text.append(word)\n",
    "    text = ' '.join(clean_text)\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "string_test = dfc['clean_text'][0]\n",
    "params = ['codec', 'numbers', 'acronyms', 'brackets']\n",
    "cleaner(string_test, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c414d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers (text):\n",
    "    words_only = ''.join([i for i in text if not i.isdigit()])\n",
    "    return words_only\n",
    "\n",
    "dfc['clean_text'] = dfc.clean_text.apply(remove_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91275101",
   "metadata": {},
   "source": [
    "#### get a list of lists of words and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Create function\n",
    "def remove_stopwords (text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    without_stopwords = [word for word in tokenized if not word in stop_words]\n",
    "    return without_stopwords\n",
    "\n",
    "dfc['clean_text'] = dfc.clean_text.apply(remove_stopwords)\n",
    "\n",
    "dfc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer() # Initiate lemmatizer\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in text] # Lemmatize\n",
    "    return lemmatized\n",
    "\n",
    "dfc['clean_text'] = dfc.clean_text.apply(lemma)\n",
    "\n",
    "dfc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08149a1c",
   "metadata": {},
   "source": [
    "# LDA : extract topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda70f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['clean_text'] = dfc['clean_text'].astype('str')\n",
    "dfc['clean_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(dfc['clean_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd6834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online')\n",
    "\n",
    "lda_vectors = lda_model.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        \n",
    "\n",
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa770672",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068de6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield TaggedDocument(list_of_words, [i])\n",
    "data_for_training = list(tagged_document(dfc['clean_text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34471ab9",
   "metadata": {},
   "source": [
    "#### Once trained we now need to initialise the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents = data_for_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65f7f5",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47675b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(data_for_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f1839",
   "metadata": {},
   "source": [
    "#### Train the Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d336c",
   "metadata": {},
   "source": [
    "## Analysing the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ef038",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = model.infer_vector(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db60cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
